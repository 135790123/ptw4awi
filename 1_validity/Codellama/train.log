12/09/2024 16:54:05 - WARNING - __main__ -   device: cuda, n_gpu: 8
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.40it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.28it/s]
12/09/2024 16:54:59 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='/root/sy/5_java_fold_datasets/train_fold_1.csv', output_dir='../results/saved_models', model_type='roberta', block_size=512, eval_data_file='/root/sy/5_java_fold_datasets/test_fold_1.csv', test_data_file='/root/sy/5_java_fold_datasets/test_fold_1.csv', model_name='codellama.bin', model_name_or_path=None, config_name='', use_non_pretrained_model=False, tokenizer_name='', code_length=256, do_train=True, do_eval=False, do_test=True, evaluate_during_training=True, do_local_explanation=False, reasoning_method=None, train_batch_size=4, eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, warmup_steps=0, seed=123456, epochs=10, effort_at_top_k=0.2, top_k_recall_by_lines=0.01, top_k_recall_by_pred_prob=0.2, do_sorting_by_line_scores=False, do_sorting_by_pred_prob=False, top_k_constant=10, num_attention_heads=12, write_raw_preds=False, use_word_level_tokenizer=False, use_non_pretrained_tokenizer=False, n_gpu=8, rq='ori', device=device(type='cuda'))
  0%|          | 0/8112 [00:00<?, ?it/s]  9%|▊         | 696/8112 [00:00<00:01, 6953.34it/s] 21%|██        | 1705/8112 [00:00<00:00, 8736.36it/s] 32%|███▏      | 2578/8112 [00:00<00:00, 6644.60it/s] 41%|████      | 3288/8112 [00:00<00:00, 6485.29it/s] 49%|████▉     | 3983/8112 [00:00<00:00, 6628.00it/s] 64%|██████▎   | 5170/8112 [00:00<00:00, 8238.34it/s] 75%|███████▍  | 6082/8112 [00:00<00:00, 8505.52it/s] 86%|████████▌ | 6955/8112 [00:01<00:00, 6216.08it/s] 95%|█████████▍| 7674/8112 [00:01<00:00, 3999.82it/s]100%|██████████| 8112/8112 [00:01<00:00, 5310.33it/s]
12/09/2024 16:55:00 - INFO - __main__ -   *** Example ***
12/09/2024 16:55:00 - INFO - __main__ -   label: 0
12/09/2024 16:55:00 - INFO - __main__ -   input_tokens: ['<s>', '▁return', '▁exports', '_', 'table', ';', '<0x0A>', '</s>']
12/09/2024 16:55:00 - INFO - __main__ -   input_ids: 1 736 29586 29918 2371 29936 13 2 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016
12/09/2024 16:55:00 - INFO - __main__ -   *** Example ***
12/09/2024 16:55:00 - INFO - __main__ -   label: 0
12/09/2024 16:55:00 - INFO - __main__ -   input_tokens: ['<s>', '▁@', 'Dep', 'rec', 'ated', '<0x0A>', 'public', '▁static', '▁final', '▁String', '[]', '▁M', 'ET', 'HO', 'D', 'H', 'AND', 'LE', '_', 'NAME', 'S', '▁=', '▁{', '▁"",', '▁"', 'get', 'Field', '",', '▁"', 'get', 'Static', '",', '▁"', 'put', 'Field', '",', '▁"', 'put', 'Static', '",', '▁"', 'invoke', 'Virtual', '",', '▁"', 'invoke', 'Static', '",', '▁"', 'invoke', 'Special', '",', '▁"', 'new', 'Invoke', 'Special', '",', '▁"', 'invoke', 'Interface', '"', '▁};', '</s>']
12/09/2024 16:55:00 - INFO - __main__ -   input_ids: 1 732 8498 3757 630 13 3597 2294 2186 1714 2636 341 2544 8187 29928 29950 9468 1307 29918 5813 29903 353 426 12633 376 657 3073 613 376 657 17046 613 376 649 3073 613 376 649 17046 613 376 9772 21287 613 376 9772 17046 613 376 9772 24780 613 376 1482 20731 24780 613 376 9772 10448 29908 3980 2 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016
12/09/2024 16:55:00 - INFO - __main__ -   *** Example ***
12/09/2024 16:55:00 - INFO - __main__ -   label: 0
12/09/2024 16:55:00 - INFO - __main__ -   input_tokens: ['<s>', '▁return', '▁((', 'NEW', 'AR', 'RAY', ')', '▁i', '1', ').', 'get', 'Type', 'code', '()', '▁==', '▁((', 'NEW', 'AR', 'RAY', ')', '▁i', '2', ').', 'get', 'Type', 'code', '();', '<0x0A>', '</s>']
12/09/2024 16:55:00 - INFO - __main__ -   input_ids: 1 736 5135 28577 1718 22800 29897 474 29896 467 657 1542 401 580 1275 5135 28577 1718 22800 29897 474 29906 467 657 1542 401 890 13 2 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016 32016
  0%|          | 0/2028 [00:00<?, ?it/s] 35%|███▌      | 710/2028 [00:00<00:00, 7098.59it/s] 84%|████████▎ | 1698/2028 [00:00<00:00, 8663.99it/s]100%|██████████| 2028/2028 [00:00<00:00, 7151.31it/s]
/root/anaconda3/envs/LLM/lib/python3.9/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
12/09/2024 16:55:09 - INFO - __main__ -   ***** Running training *****
12/09/2024 16:55:09 - INFO - __main__ -     Num examples = 8112
12/09/2024 16:55:09 - INFO - __main__ -     Num Epochs = 10
12/09/2024 16:55:09 - INFO - __main__ -     Instantaneous batch size per GPU = 0
12/09/2024 16:55:09 - INFO - __main__ -     Total train batch size = 4
12/09/2024 16:55:09 - INFO - __main__ -     Gradient Accumulation steps = 1
12/09/2024 16:55:09 - INFO - __main__ -     Total optimization steps = 20280
  0%|          | 0/2028 [00:00<?, ?it/s]  0%|          | 0/2028 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/root/sy/ptw4awi/1_validity/Codellama/codellama_main.py", line 455, in <module>
    main()
  File "/root/sy/ptw4awi/1_validity/Codellama/codellama_main.py", line 436, in main
    train(args, train_dataset, model, tokenizer, eval_dataset)
  File "/root/sy/ptw4awi/1_validity/Codellama/codellama_main.py", line 164, in train
    loss, logits = model(input_ids=inputs_ids, labels=labels)
  File "/root/anaconda3/envs/LLM/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/anaconda3/envs/LLM/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 171, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/root/anaconda3/envs/LLM/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 181, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/root/anaconda3/envs/LLM/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py", line 89, in parallel_apply
    output.reraise()
  File "/root/anaconda3/envs/LLM/lib/python3.9/site-packages/torch/_utils.py", line 543, in reraise
    raise exception
torch.cuda.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/root/anaconda3/envs/LLM/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py", line 64, in _worker
    output = module(*input, **kwargs)
  File "/root/anaconda3/envs/LLM/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/sy/ptw4awi/1_validity/Codellama/linevul_model.py", line 44, in forward
    outputs = self.encoder(input_ids=input_ids, attention_mask=input_ids.ne(1), output_attentions=output_attentions)
  File "/root/anaconda3/envs/LLM/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/anaconda3/envs/LLM/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 968, in forward
    layer_outputs = decoder_layer(
  File "/root/anaconda3/envs/LLM/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/anaconda3/envs/LLM/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 727, in forward
    hidden_states = self.mlp(hidden_states)
  File "/root/anaconda3/envs/LLM/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/anaconda3/envs/LLM/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 216, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/root/anaconda3/envs/LLM/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/anaconda3/envs/LLM/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 31.74 GiB total capacity; 30.48 GiB already allocated; 11.38 MiB free; 31.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

